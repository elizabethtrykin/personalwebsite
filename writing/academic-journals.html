<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>elizabethtrykin.com</title>

  <meta content="elizabethtrykin.com" property="og:site_name">
  <meta content="blog" property="og:type">
  <meta content="trykin.e@gmail.com" property="og:email">
  <link href="mailto:trykin.e@gmail.com" rel="me">
  <link href="https://github.com/elizabeth-trykin" rel="me">

  <link rel="icon" href="favicon.ico" sizes="any">
  <link rel="icon" href="icon.svg" type="image/svg+xml">
  <link rel="apple-touch-icon" href="apple-touch-icon.png">
  <link rel="manifest" href="manifest.webmanifest">

  <link rel="stylesheet" href="../main.min.css">

</head>

<body>
  <header>
    <nav>
      <div>
        <strong>Elizabeth Trykin</strong>
      </div>
      <ul>
        <li>
          <a href="../index.html">about</a>
        </li>

        <li class="selected">
          <a href="../writing.html">writing</a>
        </li>

        <li>
          <a href="../projects.html">projects</a>
        </li>
      </ul>
    </nav>
  </header>

  <main id="main">
    <article>
      <header id="title-block-header">
        <h2 class="title">Incentive systems in academic journals</h2>
      </header>
      <p>400 years ago, communication of scientific
        insights happened primarily via personal letters, or in-person group
        meetings. As the printing industry evolved in the 17th century, journals
        were formed to distribute hard copies of scientific research to
        individuals. The amount of government research funding began to slowly
        increase, and so did the number of researchers and institutions.</p>
      <p>The demand for funding and academic status was growing faster than
        supply, so an increasingly competitive academic environment was created.
        Metrics based on citations, publications, student numbers, grants and
        journal prestige were then created to differentiate the status of
        stakeholders in academia. Rightfully so, academics began optimizing for
        these metrics.</p>
      <p>Unfortunately, when a metric becomes a target, it ceases to be a good
        metric. As a result, academia has since seen an increase in <a
          href="https://academic.oup.com/gigascience/article/8/6/giz053/5506490#136359086">salami
          publications (2+ paper/study), ghost authorships (people excluded from
          publications), p-hacking (data manipulation), metric manipulation,
          faking research data, faking of peer reviews, and plagiarism by peer
          reviewers.</a></p>
      <p>Papers became the way scientists could prove they were performing
        valuable research. Funders, PIs, institutions and many investors began
        to <a href="https://ncses.nsf.gov/pubs/nsb20201/u-s-r-d-performance-and-funding">evaluate
          scientists based off of their papers</a>.</p>
      <p><a href="https://www.cell.com/current-biology/pdf/S0960-9822(14)00477-1.pdf">A
          survey</a> with 25,000 participating academic scientists showed that the
        mean impact factor and the total number of publications were most
        correlated with academic success, compared to any other metric. <a
          href="https://academic.oup.com/rev/article/30/1/112/6048419">Another
          survey</a> showed that nearly half of all grant proposal reviewers
        considered the number of publications and mean impact factor as highly
        important for their decision.</p>
      <p>The first person to propose a citation index, Eugene Garfield,
        intended for it to allow scientists to track citations between papers,
        and identify whether critiques were proposed for papers they were
        planning to cite. He even <a href="https://dx.doi.org/10.1001/jama.295.1.90">explicitly warned
          against</a> using citation index as a metric of evaluation in some of
        his writings.</p>
      <p>Above all, this new incentive structure brought huge benefits to
        prestigious journals. Publishers quickly realized that a subscription
        model for academic institutions brought significantly more revenue than
        one for individuals. More and more journals were created and many were
        consumed by the top-5 publishers: Elsevier, Taylor &amp; Francis,
        Wiley-Blackwell, Springer and Sage. In 2013, these 5 companies were
        publishing <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0127502">53%
          of all scientific research</a>.</p>
      <p><strong>Clearly, a monopoly with flawed incentives was
          created.</strong></p>
      <p>There are different components of academic publishing, and I’m going
        to go through each one and explain the status quo, and the incentives of
        that system.</p>
      <h2 id="quality-of-research-and-papers">Quality of research and
        papers</h2>
      <p>Over 2.5 million papers are published every year, but <a
          href="https://www.stm-assoc.org/2012_12_11_STM_Report_2012.pdf">only
          half of them</a> are ever read by anyone that isn’t an author, reviewer
        or editor. A system that is over-optimized for publishing is leading to
        a prioritization of quantity of quality, which is less than ideal from
        the perspective of valuing scientific insights/discovery.</p>
      <p>It’s common now to think that because a paper was published in
        Nature, Science or <em>insert another prestigious journal</em>, it is,
        therefore, true.</p>
      <p>Science has become more about citations than replication. Over 70% of
        papers cannot be reproduced. This can be explained by the fact that
        there is a certain margin of variability in fields like biology.
        However, most of these papers are not reproducible even accounting for a
        very large acceptable margin.</p>
      <p>Poor <a href="https://www.pnas.org/doi/10.1073/pnas.1708272114">research
          methods</a> are a very common issue among papers. Research methods are
        difficult to follow and consistently miss certain
        steps/measurements.</p>
      <p>Reviewers are pushing-forward low quality research. For example,
        editors and peer reviewers at Springer and IEEE accepted <a
          href="https://www.nature.com/articles/nature.2014.14763#/b1">hundreds of
          computer-generated papers</a>, which they then retracted a few years
        later. A reporter for Science sent a fake paper to 304 open-access
        journals, and 157 accepted it.</p>
      <p>Citation count of papers is the most common metric for determining
        journal prestige. However, <a href="https://www.science.org/doi/10.1126/sciadv.abd1705">papers that
          are cited less</a>, are often more reproducible than those that are
        cited more . Highly novel paper also take longer to accumulate
        citations.</p>
      <h2 id="peer-reviewal">Peer reviewal</h2>
      <p>Peer reviews are done for free, and are considered to be part of a
        researcher’s scientific duties. Reviews are also often single-blinded,
        meaning they are anonymous, so the reviewer has no clear incentive to
        provide fair, quality reviews.</p>
      <p>Editors try to find peer reviewers that are in the same niche of
        science as the author, the reviewer and the author are, most of the
        time, competing to solve a similar problem. For example, this problem is
        very prominent in Alzheimer research. Because so many researchers are
        trying to tackle this problem, <a
          href="https://www.statnews.com/2019/06/25/alzheimers-cabal-thwarted-progress-toward-cure/">new
          research is often shut down from publication because of reviewal, or
          competitor rejection</a>.</p>
      <p>Since scientific funding is limited per field of knowledge, reviewers
        are often more incentivized to reject a paper’s publishing, in order to
        secure more funding for themselves.</p>
      <h2 id="bias">Bias</h2>
      <p>Reviewers are biased based on who the authors of the paper are. <a
          href="https://www.nytimes.com/2018/11/05/upshot/peer-review-the-worst-way-to-judge-research-except-for-all-the-others.html">In
          a study</a>, 8 out of 9 papers that were previously accepted by
        reviewers for publishing were rejected after the names of the authors
        were changed. Papers that are produced by scientists with big names are
        much more likely to be pushed compared to others, regardless of the
        quality.</p>
      <p>Currently, 20% of scientists are also performing <a href="https://www.nature.com/articles/nature.2016.21031">up
          to 94% of
          reviews</a>, which means that 1/5th of researchers have significant
        power over what ends up being published.</p>
      <br></br>
      <p>I looked at the existing solutions tackling this problem, including
        preprints, open-access journals and sci-hub. Unfortunately, there are no
        incentives beyond participation in the “open science” movement for
        researchers to opt into these new systems.</p>
      <p>So some open questions I have: How do we scale experiment
        replication? We need to release both data and code in papers. How do
        we backtrace? How do we incentivize people to publish in alternative
        journals? How do we incentivize researchers to publish failed studies?
        Is open access science something that universities and researchers
        actually want?</p>
      <p>A lot of these statistics were gathered from one article, which I can
        no longer find as these notes were written many months ago. If you’re
        reading this and find a lot of similarities with another post you’ve
        read, please reach out so I can credit them!</p>
    </article>
  </main>
</body>

</html>